{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7s8zG3ll5sH",
    "outputId": "f7494d2c-a1ac-4212-fbf8-0548f16e335a"
   },
   "outputs": [],
   "source": [
    "!pip list | grep torch\n",
    "!pip list | grep torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbvVE9LIl_MQ",
    "outputId": "f289b206-c0f9-4f51-9914-5bb723f14d53"
   },
   "outputs": [],
   "source": [
    "#get the optimal pip installation command:\n",
    "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYrUHAafLlr_"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BwaX7XPDY2ue",
    "outputId": "ea86a3ab-7b13-49f4-fa93-d2c9c51ac851"
   },
   "outputs": [],
   "source": [
    "!pip install unsloth\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps \"unsloth[cu124-ampere-torch251] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-oK3z_5Y2fQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuzq7cf0mT7N"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "TAPWwF8imYXj",
    "outputId": "ce05ab7a-471f-4e62-f9ee-b5c2f0cd2700"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wb_token = userdata.get('wandb_api')\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune-DeepSeek-R1-Distill-Qwen-7B on ESCov',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "d9716f7001f3475f8cca31b49a90f2da",
      "cd3347954e204e6fa0c0e680849c6651",
      "9130e6ee9bed4c3ea5848884f6795ee7",
      "f60f8637a4784d1a91f2de5674a42438",
      "803c4763af8c49d1a05ae3c434b4eb10",
      "4817632bb18a4c0587ca5227dda9e6db",
      "a019310eb9734ad88825ec0352ca411a",
      "483c432b30a246399f34d9bb5457d054",
      "3df9d61add854b4898605bfbe51234d7",
      "1aa31d986736449e9665da27e1774dc6",
      "acb5ffc2e8ae40ac9b27afb11192b5a0"
     ]
    },
    "id": "G1gUUFz3mbNC",
    "outputId": "ac36bef3-f722-4bdd-91b2-388efdaedc88"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.bfloat16, #Defaults to None; use torch.float16 or torch.bfloat16 for newer GPUs.\n",
    "    load_in_4bit = True, #Enables 4-bit quantization, reducing memory use 4× for fine-tuning on 16GB GPUs. Disabling it on larger GPUs (e.g., H100) slightly improves accuracy (1–2%).\n",
    "    token = hf_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1E21JmYQZuZ"
   },
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2KRMlJDQYLh"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"jordanfan/esconv_processed\")\n",
    "\n",
    "dataset_train = dataset[\"train\"]\n",
    "dataset_test = dataset[\"test\"]\n",
    "\n",
    "# Define target strategies\n",
    "target_strategies = [\"Question\", \"Affirmation and Reassurance\", \"Providing Suggestions\", \"Restatement or Paraphrasing\"]\n",
    "\n",
    "# Filter train and test datasets\n",
    "filtered_train = dataset[\"train\"].filter(lambda example: example[\"strategy\"] in target_strategies)\n",
    "filtered_test = dataset[\"test\"].filter(lambda example: example[\"strategy\"] in target_strategies)\n",
    "\n",
    "# Split dataset into 80% train and 20% validation\n",
    "#split_dataset = filtered_train.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Assign datasets\n",
    "train_dataset = filtered_train\n",
    "val_dataset = filtered_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cc2KAcmmQtmq",
    "outputId": "8d513a63-4a36-4246-874a-085313ce0e92"
   },
   "outputs": [],
   "source": [
    "print(dataset_train.shape)  # Check dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etX81QW_Qtfs",
    "outputId": "ac48d50f-c160-49be-f494-366222b6ab81"
   },
   "outputs": [],
   "source": [
    "print(filtered_train)\n",
    "print(filtered_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgQX_pcRQyVc",
    "outputId": "99461bac-af20-4387-88d3-c9bf7ec39e93"
   },
   "outputs": [],
   "source": [
    "# Print dataset sizes\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1nxNqyhUF5P",
    "outputId": "88fb3169-5865-4fca-cd95-d54e9774fd24"
   },
   "outputs": [],
   "source": [
    "train_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHMkJAgQmj3i"
   },
   "source": [
    "Pre-Fine-Tuning Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfUIXjN5mezM"
   },
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"Below is a conversation between a student seeking mental health support and an AI counselor.\n",
    "The AI counselor provides empathetic, evidence-based responses tailored to the student's concerns.\n",
    "The student's concerns include prior conversation history and the most recent message.\n",
    "The counselor outputs a structured JSON response.\n",
    "\n",
    "### Student:\n",
    "**Conversation History:**\n",
    "{user_history}\n",
    "\n",
    "**Current Message:**\n",
    "{user_text}\n",
    "\n",
    "### Counselor Structured JSON Response:\n",
    "```json\n",
    "{{\n",
    "    \"emotion_type\": \"{emotion_type}\",\n",
    "    \"emotion_intensity (1-5)\": {emotion_intensity_initial},\n",
    "    \"problem_type\": \"{problem_type}\",\n",
    "    \"counseling_strategy\": \"{strategy}\",\n",
    "    \"answer\": \"{counselor_first}\"\n",
    "}}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UVn743cquI1",
    "outputId": "31b92702-8d83-40c9-ff20-f6df88fc3f30"
   },
   "outputs": [],
   "source": [
    "user_history = \"Hello good afternoon. I'm feeling anxious that I am going to lose my job. I hope I don't. I am on short term disability and I am not ready to go back to work yet but I do not have any job protection.\"\n",
    "user_text =\"It's not ending yet, but no my job is not protected. I live in the United States, but I have not been at my job long enough to earn protection for medical leave. you have to have been here for a year, and I started November 2020 I'm afraid that I will lose my job since I'm still on disability for the foreseeable future.\"\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_style.format(user_history =user_history,user_text=user_text,emotion_type=\"\",emotion_intensity_initial=\"\",problem_type=\"\",strategy=\"\",counselor_first=\"\")+ EOS_TOKEN], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=500,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOLIqpl9aCAb"
   },
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is a conversation between a student seeking mental health support and an AI counselor.\n",
    "The AI counselor provides empathetic, evidence-based responses tailored to the student's concerns.\n",
    "The student's concerns include prior conversation history and the most recent message.\n",
    "The counselor extracts the \"emotion_type\"(main emotion they feel going into session),\"emotion_intensity (1-5)\"(initial intensity of emotion before session),\n",
    "\"problem_type\"(topic of conversation), \"counseling_strategy\"(Aggregated strategies counselor used at the current turn) from the Conversation History and Current Message,\n",
    "and provide an \"answer\"(Counselor's first response to the user at the turn),\n",
    "then outputs a structured JSON response.\n",
    "\n",
    "### Student:\n",
    "**Conversation History:**\n",
    "{user_history}\n",
    "\n",
    "**Current Message:**\n",
    "{user_text}\n",
    "\n",
    "### Counselor Structured JSON Response:\n",
    "```json\n",
    "{{\n",
    "    \"emotion_type\": \"{emotion_type}\",\n",
    "    \"emotion_intensity (1-5)\": {emotion_intensity_initial},\n",
    "    \"problem_type\": \"{problem_type}\",\n",
    "    \"counseling_strategy\": \"{strategy}\",\n",
    "    \"answer\": \"{counselor_first}\"\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_k49Y_i7Qzea",
    "outputId": "b6456565-3d8f-4c30-92a5-af5ba0dd6f25"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    user_historys = examples[\"user_history\"]  # Student's concern history\n",
    "    user_texts = examples[\"user_text\"]  # Student's most recent message\n",
    "    answers = examples[\"counselor_first\"]  # Counselor's response\n",
    "    emotions = examples[\"emotion_type\"]  # Emotion type\n",
    "    emotion_intensity = examples[\"emotion_intensity_initial\"]  # Initial intensity\n",
    "    problem_type = examples[\"problem_type\"]  # Problem category\n",
    "    strategy = examples[\"strategy\"]  # Counseling strategy\n",
    "    texts = []\n",
    "\n",
    "    for user_history, user_text, answer, emotion, intensity, problem, strat in zip(\n",
    "        user_historys, user_texts, answers, emotions, emotion_intensity, problem_type, strategy\n",
    "    ):\n",
    "        text = train_prompt_style.format(\n",
    "            emotion_type=emotion,\n",
    "            emotion_intensity_initial=intensity,\n",
    "            problem_type=problem,\n",
    "            strategy=strat,\n",
    "            user_history=user_history,\n",
    "            user_text=user_text,\n",
    "            counselor_first=answer\n",
    "        )+EOS_TOKEN ## Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting to datasets\n",
    "format_train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "format_val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Print a formatted example\n",
    "print(format_train_dataset[\"text\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N72lLyMJBo0F"
   },
   "outputs": [],
   "source": [
    "# Keep only the 'text' column\n",
    "text_train_dataset = format_train_dataset.remove_columns([col for col in train_dataset.column_names if col != 'text'])\n",
    "text_val_dataset = format_val_dataset.remove_columns([col for col in val_dataset.column_names if col != 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJjxYaHxL8h6",
    "outputId": "b4808d3d-171f-451c-c977-cb74143b2019"
   },
   "outputs": [],
   "source": [
    "text_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJ0dJ_OYMG0U"
   },
   "source": [
    "Setting Up LoRA for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO3_KRP_L_v7"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-XJ332qMKxS",
    "outputId": "453a3987-4d5c-45c1-fd0a-1a8d696f5efa"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32, #8-64,number of trainable parameters,a larger number uses more memory and will be slower, but can increase accuracy on harder tasks..w'=w+alpha/r(AB)\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ], #select all modules to finetune\n",
    "    lora_alpha=32, #Scaling factor, a larger number will make the finetune learn more about your dataset, but can promote over-fitting. We suggest this to equal to the rank r, or double it.\n",
    "    lora_dropout=0.05, #0 is optimized\n",
    "    bias=\"none\", #\"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False, #rank stabilized LoRA\n",
    "    loftq_config=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyLpDHmXMTWd",
    "outputId": "5f4a977c-5bef-48e8-e0b8-a2d8eb13fdad"
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azX4XEkPMZPs"
   },
   "source": [
    "Configuring and Running the Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jj-oPaRmMk4q"
   },
   "outputs": [],
   "source": [
    "if hasattr(model, \"for_training\"):\n",
    "    delattr(model, \"for_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pi_8u09AMxM1"
   },
   "outputs": [],
   "source": [
    "# Ensure the entire model is in bfloat16\n",
    "model = model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXrZqgZlPLyc",
    "outputId": "bfdd2490-72b0-4e19-c6d1-06ba0fb820d0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "1930ed86e21649b0a3cd7c8e5a1825ff",
      "b847fb8af798460d8ca5f11582d80b03",
      "395eb248f9f84b229eaf11be53495366",
      "f31facbe22b24972babdfd0261812f1d",
      "d2745f2873f14d6cae183ae2fd4ed8b6",
      "22340a97853d4c538456eab4afa06033",
      "487087a1577e45d498a25cc195ac8347",
      "91772c2cd2b94a0082b95c3b98e01f5d",
      "d6780649b2114baf96fde96e937e042c",
      "46bb5485eeb34acd9bd87305ff4d2e7e",
      "8ed1c5e89ec14e25b7340759326e3488",
      "b332979d77f942219d7daf36513e091c",
      "7e4279e54f28418cb7f5c52ae460430c",
      "47019d056840471a8cf4b9084ec80788",
      "7568d97718a9418a8fc563368d14425e",
      "890e5497e0e04f7b8646c73e54f4ea00",
      "bf2b0097c5ac4b4aa3640d8acab2876a",
      "2ea6c55aa55a4648ad1d23cad24636b6",
      "ed9ea1a0a4dd48e9a697f3d512cf9e23",
      "2c561f0f7c594e3ba459cbaae2fd3b9a",
      "7ee8dec8b1324c2a86602e6028586e75",
      "030491aa357f4466beed5a94989fec75",
      "fa2ee5be5d1e4ea1af7f6f9cc026ada0",
      "c131dfe0011d40dab129e2cc71bcc227",
      "d9d860111b05468e86e4f27d46bae66a",
      "ca46587adb7f442ab438a1db9e9dcef5",
      "51425c1af64e4cfaa273dbab50208412",
      "a5f6defb6b464e048dafd4a1ea749715",
      "ab03510818b14042a8c4b726bc5eca73",
      "5b4b6895ea1f4fb28b8ba9f7bfd1ebf4",
      "300cfbc64a494eed9e8a0e6b7037fbaa",
      "63d2993468d64f908c3e049903d5bf79",
      "447b11aeedd54e468be13b742de6b474",
      "cea73ad21a7b413ab521811d7f6dc8e3",
      "ae8c67100ef64f538d2e7c5fa36436fe",
      "659207ba3ae34f0f80f196029d9c75ff",
      "eb0c775f18a84431b6dc53011eb1f0a7",
      "3a87db8487bc46c69883e15a3127aa6d",
      "d90ee36ee3054727af476aa30054e17b",
      "12e8a1c046924e559d2f1fa0aada3b0e",
      "96b19cc86e244950b377c672bf52443e",
      "0115a2392ee84395af107f30b9b331d8",
      "720e1411f74746809ce53ceed2fde31b",
      "8c95a6fe27d24b2d9544b677b013e4b5"
     ]
    },
    "id": "fIpC4Ks-MVeZ",
    "outputId": "69acc528-2711-4247-8f45-df35b5ac2211"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = text_train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2, # Noe of processes to use for loading and processing the data\n",
    "    packing = False, #Can make training 5X faster for short sequence\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2, # Number of samples per batch per GPU\n",
    "        gradient_accumulation_steps = 4, #Helps manage memory usage when increasing batch size. Accumulate gradients over 8 steps before updating weights\n",
    "        num_train_epochs = 3,\n",
    "        warmup_steps = 50,#Warmup should be ~5% of total steps for smoother training.\n",
    "        #max_steps=3403, #no need if num_train_epochs is on, 4537 examples, batchsize 4, ~1134 steps for 1 epoch\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = False, #not is_bfloat16_supported(),# Use FP16 if BF16 isn't supported\n",
    "        bf16= True, #is_bfloat16_supported(), # Use BF16 if supported (better for newer GPUs like A100)\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\", # The optimizer that will be used for updating the weights\n",
    "        weight_decay = 0.01, # Regularization term to prevent overfitting\n",
    "        lr_scheduler_type = \"linear\", # Linear learning rate decay\n",
    "        seed = 3407,\n",
    "        output_dir = \"/content/drive/My Drive/deepseek_finetuned_02\", # Where to save the model checkpoints\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "id": "xEW8pKZyMjRt",
    "outputId": "4a8a3555-eba6-4ca2-f060-3c3ac202a707"
   },
   "outputs": [],
   "source": [
    "trainer_stats=trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLxDBl_OZQ5A"
   },
   "source": [
    "Saving the Fine-Tuned Model to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNsWX8pajdoa",
    "outputId": "3e628040-966d-44e5-a64c-7acc85c09297"
   },
   "outputs": [],
   "source": [
    "save_path = \"/content/drive/My Drive/deepseek_finetuned_03\"\n",
    "model.save_pretrained(save_path) #this only save a small subset of parameters that were fine-tuned (like q_proj, v_proj, etc.)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print({save_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "cd906f1268be45459921b536768d983d",
      "fcf8b34d988f47e4bf0f1f228abe13cf",
      "c14e58ac9bfe4dc79f673ff07edb9d14",
      "c8ff836dd5c64ed8bdddcd6183c5d8f4",
      "3af97fe9321547d3ba15ef0ca742ec18",
      "711641e4d0514212bd57dc1b55217761",
      "275c3d98550d4e83b60926a6bfd2f60f",
      "885b005439a54e3c9fa5df16b336c725",
      "947fc713a5dc4e189bb3bdcae3bc9b7c",
      "48202380f13840e2ae018ea207f6f05a",
      "8b53e1adc80d4e7498a6ee349dec240a"
     ]
    },
    "id": "PWs8lSM5nQCv",
    "outputId": "6dab2023-70a9-42ae-82a8-c8d0935a7200"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"  # base model\n",
    "save_path = \"/content/drive/My Drive/deepseek_finetuned_03\"  # LoRA checkpoint path\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = PeftModel.from_pretrained(base_model, save_path)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "final_save_path = \"/content/drive/My Drive/deepseek_finetuned_full_03\"\n",
    "model.save_pretrained(final_save_path)\n",
    "tokenizer.save_pretrained(final_save_path)\n",
    "print({final_save_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362,
     "referenced_widgets": [
      "a00686ad53a74697bfebb9de97f1b176",
      "a6f550e72f7b4d6598121691fe29b77c",
      "c36f607e7311402b8997b97013938d1c",
      "34a154838803420c9273c3af317afaba",
      "c4ac660d0a0f4041960c2ff7a4e50e67",
      "ef1b76ca22ec4b12a7acce007b8b3965",
      "27347b5d4e0740be83417d2eee619304",
      "d46f8df725a64374ab950577d3f2430a",
      "689ac1f3057e4215bd60ff3350bb54f8",
      "66b766a34928482dac283fe48e219acc",
      "40226d27521c48cdaa348f5d442bccce",
      "cfcd82a32fe24571969b5b295cb32419",
      "f6e9ed4a6ec941d8943357fcc2db00ca",
      "89063f66632d451caab3d67c85a205c5",
      "17a060785c074cb59039482035a968b8",
      "66b1d45c9379418898b1e5dda5438285",
      "2ead335373a045a78d380f7bff3bb105",
      "ea1542c25f4e4447bb807d722c6ef536",
      "23b0af2832704bbf8acc5392959e9cb1",
      "b7d4db4db32e478eb5986d3f1ccb330e",
      "57c0b432d5a045068ecd71d9cc1614c3",
      "f32f1ad669ef4c15b0216452e7d87529",
      "ad3598b0aa3e4706abf16a8420b1908f",
      "9d1e539694254f689d74001d6555c661",
      "ddeec5c543784adb86b630369a52ec85",
      "7ab90b2a4ad94696a40b9bb3bb121575",
      "eefab8c0aca14bd4a70836c4d16cf7db",
      "07f5b0e53ecc4eaf90e1f57e5a571f2e",
      "8a06ef5eea3443f6848e3ca7c2b9bbb2",
      "c296fe158f3949aaaf4391ead973ac90",
      "2e006bb6ae8d4752b17331ab58a26648",
      "795a966a15094f78bc04398264e828c5",
      "ab6152f0cc3d4b66ac27e42d15f63451",
      "8200cf43f49f4724b4fb95acbe597849",
      "db4f302f0ccc44ae998b93c89ad24625",
      "39b867c0b75346e9913d45143256508d",
      "e5c64a937b794054a30710d968dbac06",
      "9aa371e9ef194857aba7c1955fa07a70",
      "2e48f6533d794f1592d25be462bd6fbc",
      "e5321a2389a24359b6b8ebe1369e68d3",
      "906f4a7279b049c087244a87eeff861d",
      "86eff339f1d64220866c9e12efc6a266",
      "7adf3162e9144d2eada3b38392312765",
      "7bdabd73443e452c891ce52b31cb5b76",
      "6ee48ab026194c4cb447eadcc03df098",
      "e6d6b395c27e4450a753ed4267c81810",
      "428d71ac9fbe49fdad4a4afb8f6985c8",
      "e922f958363a4f8ba2e1bc3c63ee80be",
      "209dafc7b0824eac97c69379e1a0c1c1",
      "81b08016ab6b4269b43e9ecfe6c7f63a",
      "1af05064eab94c048457fe43ebb38c97",
      "118709fb0d574b568135843ad621ecd0",
      "1af6f124dae84e0cb8792166b4d3ae4e",
      "5cb6e405cf2f4280927c752d751ac889",
      "8fc05f58e50a482a88ed8f697e5ad2e5",
      "1ad39f7b0f6045afb01734bbcc387a2f",
      "df3ad0f85c7e4443ad121aefdd77cf6d",
      "830419d6d94147af8d154262ee3edd24",
      "9ebedcb7a5d6456e97ae13b0b9079ed9",
      "0c4326168ba44b73ad03dd90c78d300a",
      "94e72971295d4257af7ca67434d3f5ab",
      "f2ce60ec57234227bcf95f6516a52464",
      "0c4a1a865a374430a4a2f7dffdcba4c2",
      "abc34bbfd9984e2aa9a026312865550e",
      "25fdddb1e5914e63b1291735b5d50623",
      "b2133ac8c7064855b222263f72ecec30",
      "619c5854fd7e406fa96b00589e2f8281",
      "ab491874a3c34b7eb0ea545d5147c792",
      "5b42c0655e5b4c8eb2b74ca029a9ec56",
      "25c35fa6ed514f309c1ba815b9928b04",
      "c7fb205d14d94c11851cccd029e0f8e0",
      "e42ecd6421f6434c897b91085ffe6755",
      "3572ed9fbf1f44ee83d179043cb50fb9",
      "2366fb52ec6c46dcb3fa706a60300318",
      "563eae51829e464997892d2f4b49d7b5",
      "45546a22e0a444d28cc61b30834e7fd3",
      "99a8d483999f4cc3bcc70e7bdbcf35d3",
      "e197df6717404ffbbe1efa6b0d105540",
      "8ebe0a00630544b0847bf040b9c4a001",
      "fad24a79cfd44764b0b75ee179eb692a",
      "02f3ccdab81d4c39973bfbe0b01a440d",
      "f3cc1614b8b9437f9f922b41e561b3dd",
      "de79c7a63a79484981bc6d74bdda9be1",
      "ef64d5f0163e4932a051e2c04defa102",
      "3db19c10e5294ee8a6c981595422d6a1",
      "ee139be9597c4883911cae9d9d14e67f",
      "02f63b4aae964ff581046aee943485ce",
      "dfeba12ca0e0426192f7fc85bb7d0c24"
     ]
    },
    "id": "rORh4ea4ZRMw",
    "outputId": "208b986a-dfec-46c6-b6e8-82d9f24472e5"
   },
   "outputs": [],
   "source": [
    "# Save to Huggingface\n",
    "model.push_to_hub(\"andong90/DeepSeek-R1-Distill-Qwen-7B-student-mental-health-json\", token=hf_token)\n",
    "tokenizer.push_to_hub(\"andong90/DeepSeek-R1-Distill-Qwen-7B-student-mental-health-json\", token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-y_MamsP4lc"
   },
   "source": [
    "Reload Model when restart notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "5b5f10bee8624b39b731a3667f80dcb8",
      "979e8fd3d8364d73a3d5ed368c55d157",
      "87079c017a2a4664bb7915b449223598",
      "a7cf6bd7318d45298a927cc1c57fff8c",
      "f96587c67d70481489a4307b0f98fb43",
      "1cad72b324534e10a2ddcc9a579ceb92",
      "63c00146c1484756825bed78cbc44d13",
      "b82268896d81496d8a6f61ef3a6e0a14",
      "c9477ef0bdf3407f8b68bc5b99b241e8",
      "be29d327141647b0a5b7b7532c947a1e",
      "826cee670c6d4d258de471a1484164b5",
      "57d50473d4d34730a25aca512b32114e",
      "384b3d9ac16f4571960b8334fc2450de",
      "c743a4d58d1f4d96b3fa5a5855cfa4b7",
      "4ab6b5df1f324812a1a5393a1f2b1558",
      "f7d818a68b7b428c9312e0480bc4c99b",
      "7d7978f8e5e742aa99396ad1cb707157",
      "d7077b7bc4234035beebe77242e14e04",
      "ca42486dbcd24cea89370870b7c20f6a",
      "911623282350481ea8784c49c5bfb499",
      "8d13958c41f84bfcaaf08191f9604335",
      "aa5f10efc9a34a89805ecc7b7d9470d0",
      "74f95d3c24fe432c9db876931351f3b5",
      "447bf384a9b04766bb076de6d28c8cc1",
      "6747301a6a9542b095ef53dc68aa6170",
      "d4f67ae77dac4daca46bd2f868b072dd",
      "20b1f06e3c954e05975976006c4ed231",
      "7a6c65e7119a4c3ea132435c340a7616",
      "951ea11601e24640bcd3bb093b06b5d4",
      "fb83de2288c84e5fa4bbfa866d6c02a6",
      "f071b6379fe342b69aba0c2b309815d8",
      "e3ffd864ea32444398e02ac7f45a15ea",
      "36b59291a23942c196889fa0f4a7fa36",
      "be04ce345cfb4728a9c2bffbdf791f40",
      "840989ef50da46869209d584a251f8bf",
      "2fa1e0846c5c4248aa3409b3387cfcf6",
      "f940b84fe9084c88a01960200821d09c",
      "788e954f21e2458e8edc63fa985b6f0a",
      "603e0fb258f7412c9f7f32e34b85d2d6",
      "00bda8bb8b4d4ada9567601c87c97c8d",
      "dfc9996629864817a7dc3e21b26710ee",
      "dc17d3fe6dd64ea9a672ca778bcb36e5",
      "26ec352c95fe4b3a8cde85b05f0c412c",
      "9f8ebf43f9ad4ee98d3a3ab46ea33dc3",
      "f34f956bd03c45d482fcc4d685c6c571",
      "929158cd61764271966cd2f1ec2a8433",
      "33c2b410e77a4abfbe0add16f4228780",
      "127b0510079e473ea6a4d694f27e1b38",
      "eec9231194924d63bd6e40dd70d30043",
      "20412deecaca4557bcea216f9bddc55f",
      "cdbc2e03fc7f4ad1898e43b859443975",
      "531fa36415f9410d8469799167872fe8",
      "47c63c8d088b4f3da8db8079cbaaccdb",
      "9bdd51402799451eb5b290309a068075",
      "e988bafdba91464ea6a5c71bb99a43c9",
      "1e90af52d673485694ff7f210b73428e",
      "d8f944c0761f4d3f896ee71c65984fbd",
      "3948aafb350e4edaa05fd90b7b748e43",
      "4720658911b04aa1ac32b5aba0aa9444",
      "24d056e7e71b44e488a89e5b7d312eb3",
      "066d39a0be5f442e8fc260556c6329c3",
      "9a40357bf75a4364b043f3ac2dd63637",
      "b816ac6511164c47b2607ef3b753bdb1",
      "bf74b85d03e246c4b3dc1b3c03634044",
      "296e859adf374ad98e2f319054b62a99",
      "e192f623d873475dbd0302a55a15346b"
     ]
    },
    "id": "rI6-r0ATP0py",
    "outputId": "1f7ac606-035c-486c-df7f-80d30b88be43"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"andong90/DeepSeek-R1-Distill-Qwen-7B-student-mental-health-json\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mqrYRdIPvNA"
   },
   "source": [
    "Test on few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asGZHhT_P0mp"
   },
   "outputs": [],
   "source": [
    "prompt_test = \"\"\"Given a student's Conversation History and Current Message, extract the relevant metadata, including emotion type, emotion intensity (1-5), problem type, and counseling strategy.\n",
    "Then answer the student's Current Message as a counselor based on the metadata. Keep it concise but affirmative.\n",
    "The counselor must return a Structured JSON Response with these fields: \"emotion_type\",\"emotion_intensity\", \"problem_type\", \"counseling_strategy\",\"answer\".\n",
    "\n",
    "### Student:\n",
    "**Conversation History:**\n",
    "{user_history}\n",
    "\n",
    "**Current Message:**\n",
    "{user_text}\n",
    "\n",
    "### Counselor Structured JSON Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSK2D_o2RwIZ"
   },
   "outputs": [],
   "source": [
    "#You are an AI trained in counseling techniques.\n",
    "'''Given a student's Conversation History and Current Message, extract the relevant metadata, including emotion type, emotion intensity (1-5), problem type, and counseling strategy.\n",
    "Then answer the student's Current Message as a counselor based on the metadata. Keep it concise but affirmative.\n",
    "The counselor must return a Structured JSON Response with these fields: \"emotion_type\", \"sadness\",\"emotion_intensity\", \"problem_type\", \"counseling_strategy\",\"Providing Suggestions\",\"answer\".'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYZHzVuHXNnv",
    "outputId": "138f28bb-0d4e-4a6e-9b12-d9db7ee8926e"
   },
   "outputs": [],
   "source": [
    "val_dataset[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVVJFHGbdur6",
    "outputId": "70b6843f-f217-4f11-8d41-f0d333b52ab9"
   },
   "outputs": [],
   "source": [
    "user_history=val_dataset[6]['user_history']\n",
    "user_text=val_dataset[6]['user_text']\n",
    "print(\"This the user history:\",user_history)\n",
    "print(\"This is the current question:\",user_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZ5rYmYzdpCl",
    "outputId": "37817b70-78f4-4f12-d417-8a3dd0d9f2ad"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPvbKTgvXDon",
    "outputId": "fb7c69f7-7ae4-490f-d7cd-e30be7f5debf"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_test.format(user_history=user_history,user_text=user_text)], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=250,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.6, # deepseek doc recommended 0.6 to balance creativity and coherence, avoiding repetitive or nonsensical outputs.\n",
    "    top_p=0.9,  # Reduces repeated phrases\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "print(response)\n",
    "#print(response[0].split(\"### Counselor Structured JSON Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3vm8rOsfEwH"
   },
   "source": [
    "Try another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OaoQKiffG6Y",
    "outputId": "f2e54c80-67b6-4cad-888f-242060d9aac9"
   },
   "outputs": [],
   "source": [
    "val_dataset[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ik6mP_uOfG2E",
    "outputId": "b10a4d2b-7d2c-4f20-e6c7-e33b8c70f7a2"
   },
   "outputs": [],
   "source": [
    "user_history=val_dataset[15]['user_history']\n",
    "user_text=val_dataset[15]['user_text']\n",
    "print(\"This the user history:\",user_history)\n",
    "print(\"This is the current question:\",user_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXg_bSd-SDgn",
    "outputId": "c4e548e8-e56b-4514-fe0c-7c83de1aeaa1"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_test.format(user_history=user_history,user_text=user_text)], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.6, # deepseek doc recommended 0.6 to balance creativity and coherence, avoiding repetitive or nonsensical outputs.\n",
    "    top_p=0.9,  # Reduces repeated phrases\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "print(response)\n",
    "#print(response[0].split(\"### Counselor Structured JSON Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BF0MD6FIfGuz",
    "outputId": "199959f2-1715-4ceb-e16e-ddb9292885b2"
   },
   "outputs": [],
   "source": [
    "print(response.split(\"### Counselor Structured JSON Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aep7JP-8l7E"
   },
   "source": [
    "Another exmaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_FBAPpwgidc",
    "outputId": "7e88fc7a-14af-4453-cb50-8c233ac67be4"
   },
   "outputs": [],
   "source": [
    "val_dataset[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71JktwCTgiZD",
    "outputId": "eec272bb-ab10-4ee8-db53-c7af19f527cd"
   },
   "outputs": [],
   "source": [
    "user_history=val_dataset[25]['user_history']\n",
    "user_text=val_dataset[25]['user_text']\n",
    "print(\"This the user history:\",user_history)\n",
    "print(\"This is the current question:\",user_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vib3jdT9giQc",
    "outputId": "eec8f112-c47b-4521-f3cb-019a8123cbe2"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_test.format(user_history=user_history,user_text=user_text)], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.6, # deepseek doc recommended 0.6 to balance creativity and coherence, avoiding repetitive or nonsensical outputs.\n",
    "    top_p=0.9,  # Reduces repeated phrases\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "print(response[0].split(\"### Counselor Structured JSON Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7FwWQ9b-Ed2"
   },
   "source": [
    "4th test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0GdZoo7-EIy",
    "outputId": "391f6983-3fef-49b0-fb21-657d9f79e8b2"
   },
   "outputs": [],
   "source": [
    "val_dataset[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1Hzhvr2-EEB",
    "outputId": "a88ef958-50d1-4130-a1e2-5fc07ad03844"
   },
   "outputs": [],
   "source": [
    "user_history=val_dataset[45]['user_history']\n",
    "user_text=val_dataset[45]['user_text']\n",
    "print(\"This the user history:\",user_history)\n",
    "print(\"This is the current question:\",user_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Iq2ngeL-DqY",
    "outputId": "deab1f99-562b-45c9-ab2b-9d43c537b81d"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_test.format(user_history=user_history,user_text=user_text)], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=300,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.6, # deepseek doc recommended 0.6 to balance creativity and coherence, avoiding repetitive or nonsensical outputs.\n",
    "    top_p=0.9,  # Reduces repeated phrases\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "print(response[0].split(\"### Counselor Structured JSON Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5pHNOlQACd7"
   },
   "source": [
    "5th test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mbt_vw2nAB76",
    "outputId": "ee9ee2e7-23ff-4f01-afaf-2306b699b0ed"
   },
   "outputs": [],
   "source": [
    "val_dataset[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaoGpUn8AB4z",
    "outputId": "033c9dcf-df1f-4104-8a24-2b7551d6ba71"
   },
   "outputs": [],
   "source": [
    "user_history=val_dataset[50]['user_history']\n",
    "user_text=val_dataset[50]['user_text']\n",
    "print(\"This the user history:\",user_history)\n",
    "print(\"This is the current question:\",user_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMGYyhASAB1r",
    "outputId": "d8e47242-d896-47e2-8ac4-9ce915ad4b38"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_test.format(user_history=user_history,user_text=user_text)], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.6, # deepseek doc recommended 0.6 to balance creativity and coherence, avoiding repetitive or nonsensical outputs.\n",
    "    top_p=0.9,  # Reduces repeated phrases\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "print(response[0].split(\"### Counselor Structured JSON Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B5BnNN_KzPy"
   },
   "source": [
    "6th Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvP6QEZjKy-s",
    "outputId": "306e96fb-a226-4698-8942-d6b1d4267758"
   },
   "outputs": [],
   "source": [
    "val_dataset[63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbLifEptKy41",
    "outputId": "20e493ec-383f-410a-b72c-dbfcbef5ce94"
   },
   "outputs": [],
   "source": [
    "user_history=val_dataset[63]['user_history']\n",
    "user_text=val_dataset[63]['user_text']\n",
    "print(\"This the user history:\",user_history)\n",
    "print(\"This is the current question:\",user_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyOkVF4MKy1K",
    "outputId": "1abd95e8-470b-4b21-d37b-ef7caa6dc8a3"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_test.format(user_history=user_history,user_text=user_text)], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.6, # deepseek doc recommended 0.6 to balance creativity and coherence, avoiding repetitive or nonsensical outputs.\n",
    "    top_p=0.9,  # Reduces repeated phrases\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "print(response[0].split(\"### Counselor Structured JSON Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehG1yI-DL5Oq",
    "outputId": "7d182570-463f-41d3-e93c-9a0fe8cf53ad"
   },
   "outputs": [],
   "source": [
    "val_dataset[73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "682W4sqXL5KY",
    "outputId": "c9ea65ae-9ca0-414e-8efe-47c207f2cb1a"
   },
   "outputs": [],
   "source": [
    "user_history=val_dataset[73]['user_history']\n",
    "user_text=val_dataset[73]['user_text']\n",
    "print(\"This the user history:\",user_history)\n",
    "print(\"This is the current question:\",user_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gb9raa9QL5G_",
    "outputId": "5cd72cb3-abe1-4396-ee2f-55998d89390c"
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer([prompt_test.format(user_history=user_history,user_text=user_text)], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.6, # deepseek doc recommended 0.6 to balance creativity and coherence, avoiding repetitive or nonsensical outputs.\n",
    "    top_p=0.9,  # Reduces repeated phrases\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "print(response[0].split(\"### Counselor Structured JSON Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfDQPVVn-stI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
