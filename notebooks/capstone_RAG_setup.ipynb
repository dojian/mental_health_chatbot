{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMXoWzckFJGN"
   },
   "source": [
    "# RAG\n",
    "\n",
    "This notebook is to:\n",
    "1. Load documents\n",
    "2. Split documents into chunks\n",
    "3. Save documents and chunks to `data` directory\n",
    "\n",
    "References\n",
    "* [Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)\n",
    "* [Hallucination Elimination Using Acurai](https://arxiv.org/pdf/2412.05223)\n",
    "* [s1: Simple test-time scaling](https://arxiv.org/pdf/2501.19393)\n",
    "* [Implementing Contextual Retrieval in RAG pipline](https://medium.com/the-ai-forum/implementing-contextual-retrieval-in-rag-pipeline-8f1bc7cbd5e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXNH-C9Pgl5B"
   },
   "outputs": [],
   "source": [
    "# Rag Setup\n",
    "%%capture\n",
    "!pip install -q sentence_transformers\n",
    "!pip -q install langchain\n",
    "!pip -q install langchain-qdrant\n",
    "!pip install langchain_community\n",
    "!pip install --upgrade --quiet chromadb bs4 qdrant-client\n",
    "!pip install langchainhub\n",
    "!pip install -U langchain-huggingface\n",
    "!pip install --upgrade --quiet  pymupdf\n",
    "\n",
    "### Contextual Retrieval Installations\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf==3.20.3 datasets huggingface_hub hf_transfer\n",
    "!pip install --no-deps unsloth\n",
    "!pip install transformers\n",
    "\n",
    "### Contextual BM25\n",
    "!pip install elasticsearch\n",
    "!pip install rank_bm25\n",
    "!pip install faiss-cpu\n",
    "!pip install flashrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ot7IPbFBD-Th"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oC1Dp_C2i5ZV",
    "outputId": "73c4d797-1bc3-40c6-d155-b9f55870376e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import threading\n",
    "import time, locale\n",
    "from google.colab import drive\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "### Contextual Retrieval Imports\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "### BM25\n",
    "import hashlib\n",
    "import os\n",
    "import getpass\n",
    "from typing import List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.retrievers import ContextualCompressionRetriever,BM25Retriever,EnsembleRetriever\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers.embeddings_redundant_filter import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import flashrank\n",
    "\n",
    "### TODO: Parallelize inferencing\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_groq import ChatGroq\n",
    "# from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "# from transformers import pipeline, BitsAndBytesConfig\n",
    "# from langchain_huggingface import HuggingFacePipeline\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "# from langchain import PromptTemplate, LLMChain\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_text_splitters import CharacterTextSplitter\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain import hub\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_community.vectorstores import Qdrant\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "droUrTOGEsiz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i3gt0n8EGs-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44cD1PacjBJa"
   },
   "outputs": [],
   "source": [
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4SzLHeYyXyA"
   },
   "outputs": [],
   "source": [
    "##### Hyper Parameters\n",
    "# gte-Qwen2-1.5B-instruct\n",
    "embedding_base = \"multi-qa-mpnet-base-dot-v1\"\n",
    "embedding_bge = \"BAAI/bge-large-en-v1.5\"\n",
    "# model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\"\n",
    "chunk_size = 256\n",
    "chunk_overlap = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AjoIJbIjD_z"
   },
   "outputs": [],
   "source": [
    "# Embedding Model\n",
    "%%capture\n",
    "base_embeddings = HuggingFaceEmbeddings(model_name=embedding_bge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uTlRFRoEjgOC",
    "outputId": "21206267-5922-4b5b-f8a2-6d37d4fe731c"
   },
   "outputs": [],
   "source": [
    "# Folder\n",
    "drive.mount('/content/drive')\n",
    "folder_location = '/content/drive/MyDrive/capstone/RAG_items'\n",
    "list_of_documents = os.listdir(folder_location)\n",
    "save_file_location = os.path.join(folder_location, 'data')\n",
    "if not os.path.exists(save_file_location):\n",
    "  os.makedirs(save_file_location)\n",
    "pdf_file_path = 'pdfs_to_ingest.txt'\n",
    "extra_pdf = 'OSW Self-Care Workbook.pdf' # TODO: still need to ingest\n",
    "list_of_documents.remove(pdf_file_path)\n",
    "list_of_documents.remove(extra_pdf)\n",
    "\n",
    "### TXT location\n",
    "txt_files_to_ingest_path = os.path.join(folder_location, 'txt_files_to_ingest')\n",
    "txt_files_to_ingest = os.listdir(txt_files_to_ingest_path)\n",
    "\n",
    "### PDFs directory location for the urls\n",
    "pdf_files_to_ingest_path = os.path.join(folder_location, 'pdfs_to_ingest.txt')\n",
    "pdf_urls = []\n",
    "with open(pdf_files_to_ingest_path, 'r') as f:\n",
    "    pdf_urls.append(f.read().splitlines())\n",
    "pdf_urls = list(set(pdf_urls[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FABQsAA1I3fp",
    "outputId": "5b11889d-3e9d-488b-b461-8d28996ca884"
   },
   "outputs": [],
   "source": [
    "pdf_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9ZJhQAHCkwg"
   },
   "outputs": [],
   "source": [
    "class GetDocumentLoader:\n",
    "  def __init__(self):\n",
    "    self.text_splitter = None\n",
    "    self.global_doc_number = 0\n",
    "    self.documents = []\n",
    "    self.chunks = []\n",
    "\n",
    "  def get_text_splitter(self):\n",
    "    return self.text_splitter\n",
    "\n",
    "  def get_global_doc_number(self):\n",
    "    return self.global_doc_number\n",
    "\n",
    "  def get_documents(self):\n",
    "    return self.documents\n",
    "\n",
    "  def get_chunks(self):\n",
    "    return self.chunks\n",
    "\n",
    "  def set_text_splitter(self, chunk_size, chunk_overlap):\n",
    "    self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "  def load_from_file(self, path_to_file):\n",
    "    with open(path_to_file, 'rb') as f:\n",
    "      data = pickle.load(f)\n",
    "      self.documents = data['documents']\n",
    "      self.chunks = data['chunks']\n",
    "      self.global_doc_number = data['global_doc_number']\n",
    "      self.text_splitter = data['text_splitter']\n",
    "      print(f'Loaded {len(self.documents)} documents')\n",
    "      print(f'Loaded {len(self.chunks)} chunks')\n",
    "\n",
    "  def save_to_file(self, path_to_file):\n",
    "    data = {\n",
    "        'documents': self.documents,\n",
    "        'chunks': self.chunks,\n",
    "        'global_doc_number': self.global_doc_number,\n",
    "        'text_splitter': self.text_splitter\n",
    "    }\n",
    "    with open(path_to_file, 'wb') as f:\n",
    "      pickle.dump(data, f)\n",
    "    print(f\"Saved {len(self.documents)} documents\")\n",
    "    print(f\"Saved {len(self.chunks)} chunks\")\n",
    "\n",
    "  def data_txt_separator(self, file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8-sig') as f:\n",
    "      lines = f.readlines()\n",
    "      metadata = {}\n",
    "      for line in lines[:6]:\n",
    "        if \":\" in line:\n",
    "          key, value = line.strip().split(\":\", 1)\n",
    "          metadata[key.strip()] = value.strip()\n",
    "          metadata['doc_num'] = self.get_global_doc_number()\n",
    "      self.global_doc_number += 1\n",
    "      content = \"\".join(lines[len(metadata):]).strip()\n",
    "      return Document(page_content=content, metadata=metadata)\n",
    "\n",
    "  def load_text_documents(self, directory_path):\n",
    "    with tqdm(total=len(os.listdir(directory_path)), desc=\"Processing files\") as pbar:\n",
    "      for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('txt'):\n",
    "          file_path = os.path.join(directory_path, filename)\n",
    "          doc = self.data_txt_separator(file_path)\n",
    "          self.documents.append(doc)\n",
    "          pbar.update(1)\n",
    "        print(f'Loaded: {self.get_global_doc_number()}')\n",
    "      print(f'Complete Loading of : {self.get_global_doc_number()}')\n",
    "\n",
    "  def load_pdf_documents(self, url_paths: list) -> list:\n",
    "    with tqdm(total=len(url_paths), desc=\"Processing URLs\") as pbar:\n",
    "      for url_path in url_paths:\n",
    "        loader = PyMuPDFLoader(url_path)\n",
    "        pages = loader.load()\n",
    "        for page_num in range(len(pages)):\n",
    "          page = pages[page_num]\n",
    "          page.metadata['doc_num'] = self.get_global_doc_number()\n",
    "          self.documents.append(page)\n",
    "          pbar.update(1)\n",
    "          self.global_doc_number += 1\n",
    "        print(f'Loaded: {self.get_global_doc_number()}')\n",
    "    print(f'Global doc number after file: {self.global_doc_number}')\n",
    "\n",
    "  def index_splitter_doc_chunks(self, chunk_size, chunk_overlap):\n",
    "    self.set_text_splitter(chunk_size, chunk_overlap)\n",
    "    splits = self.text_splitter.split_documents(self.documents)\n",
    "    for idx, text in enumerate(splits):\n",
    "      text.metadata['chunk_num'] = idx\n",
    "      text.metadata['chunk_id'] = f\"doc_{text.metadata['doc_num']}_chunk_{text.metadata['chunk_num']}\"\n",
    "    self.chunks = splits\n",
    "    print(f'number of splits/chunks: {len(self.chunks)}')\n",
    "\n",
    "  # def_index_splitter_list_doc_chunks(self, chunk_size, chunk_overlap):\n",
    "  #   self.set_text_splitter(chunk_size, chunk_overlap)\n",
    "  #   splits = self.text_splitter.split_documents(self.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4MCTsiS3esa"
   },
   "outputs": [],
   "source": [
    "dl = GetDocumentLoader()\n",
    "date_now = datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6E5ipGVJKc0"
   },
   "outputs": [],
   "source": [
    "# ### Load documents and chunks from file\n",
    "# if os.path.exists(os.path.join(save_file_location, 'data--contextual-retrieval-2025-02-11-28.pkl')):\n",
    "#   dl.load_from_file(os.path.join(save_file_location, 'data--contextual-retrieval-2025-02-11-28.pkl'))\n",
    "# else:\n",
    "#   print('file does not exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1fcgWnKGmgH"
   },
   "outputs": [],
   "source": [
    "# ### Load documents and chunks\n",
    "# print('Start the loading process')\n",
    "# dl.load_text_documents(txt_files_to_ingest_path)\n",
    "# dl.get_global_doc_number()\n",
    "# print(f\"Amount of Text Documents: {len(dl.get_documents())}\")\n",
    "# dl.load_pdf_documents(pdf_urls)\n",
    "# print(f\"Amount of PDF Documents: {len(dl.get_documents())}\")\n",
    "# print(f\"Total Documents: {dl.get_global_doc_number()}\")\n",
    "\n",
    "# # split documents up into chunks and index each chunk\n",
    "# print('Start chunking process')\n",
    "# dl.index_splitter_doc_chunks(chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxX-r6gF3epc"
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists(os.path.join(save_file_location, f'data-doc-chunks-{date_now}.pkl')):\n",
    "#   dl.save_to_file(os.path.join(save_file_location, f'data-doc-chunks-{date_now}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pV22i2kJB7gT"
   },
   "outputs": [],
   "source": [
    "### Load new txt in\n",
    "text_2025_02_14 = \"/content/drive/MyDrive/capstone/RAG_items/2025_02_14_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OfRL1CysB7ds",
    "outputId": "7c536eb3-6a04-4354-b2dc-ab1772a3f852"
   },
   "outputs": [],
   "source": [
    "dl.load_text_documents(text_2025_02_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rc6E4PIB7bN",
    "outputId": "ff6071a2-052e-45e4-e7b7-3004f96f7f45"
   },
   "outputs": [],
   "source": [
    "### Load new pdfs in\n",
    "dl.load_pdf_documents(['https://careerdevelopment.princeton.edu/sites/g/files/toruqf1041/files/documents/networking_guide-oct._2020.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rAMdBiUHEUpB",
    "outputId": "8d8330b7-6c1d-48b3-f05a-2988e1793c0a"
   },
   "outputs": [],
   "source": [
    "pprint(dl.get_documents()[29].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmJDQPR2Eyfb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pAihlCnDyd7",
    "outputId": "3ffa8f71-56c3-44b8-948e-cb6df9dcbead"
   },
   "outputs": [],
   "source": [
    "### Load new documents into chunks\n",
    "print('Start chunking process')\n",
    "# dl.index_splitter_doc_chunks(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "len(dl.get_chunks())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dizqrcc3DIB8",
    "outputId": "8defb876-de79-42b4-a6dc-37ba3ba0f558"
   },
   "outputs": [],
   "source": [
    "### Save new data into RAG\n",
    "if not os.path.exists(os.path.join(save_file_location, f'data-doc-chunks-{date_now}.pkl')):\n",
    "  dl.save_to_file(os.path.join(save_file_location, f'data-doc-chunks-{date_now}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QC_yb-2uDH-0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxL2M6Kd455y"
   },
   "outputs": [],
   "source": [
    "# dl.get_chunks()[56].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c0cf2c7d92754da48512b91574f813c6",
      "10fef456719645438556ef543d178f6e",
      "20469b0d19db4bd7921a03c575a10786",
      "5de217943e654a08b3661ccecc6266a2",
      "69d7aa1fd118420185024a1d838e5b66",
      "22a6b600961c4db0b1ca02fc2ff73521",
      "aa75720ceae944a49d28449c4ea05cce",
      "3adafa834a2a4a79bf424319356806e5",
      "5daca66885cd4cb3b86fc185eea7c9e5",
      "cd92cd5611554d6d9d362f1cf982ebf3",
      "b2f3412c17e242318a70bb074cd87a4f",
      "d8744bdeb7a44534b9b92c48fa86b73f",
      "e621160dcef44b85a072ade4b7bfa7c8",
      "bbb16ede2a5e45c28fc915b460fc80be",
      "c37c7f3cae9744879535aa2027b1b08e",
      "6065b60040934284b59a4a90f99318b3",
      "02b2cf5fd83e466b9c9f8b8c4cc39497",
      "0b5f6d90333449e0af264074c2dd53b9",
      "e73ccd1a51a2496ca447acfd09858121",
      "3593afce063e4d868fa2569ef66732ca",
      "9bda53f9f4f04f0da29a27bc22157ea4",
      "e5f4ba54825c47dab2cdcef86f0e50e4",
      "cc8b0fa4e9304969aeac18e813af8101",
      "74865718306b40a68ab6061ab2d97938",
      "7aae23b76b5547f88130e0e5afdbfc8f",
      "e6a78f76be5248f5ac26e2ac6a3f82b9",
      "11ffce0428e24acb80f1f2af4984ffa3",
      "b6f604eb4f9a4a1996f72f745b67e224",
      "0ac583dcbbe3494483598a2f032d3309",
      "3a9ebdfca8954acf8db72cc54f714d2e",
      "4212bd750f2642a9a3c1d24e2ff79609",
      "97d68c49b9764458bf7d3378c31ad8c6",
      "fd120b1e1a8d43c6a3d3f59e30fcb2bf",
      "8d7a7a6b27b1464e84c2b53213695119",
      "959a5a4001974027b984e54cdf3f8bfa",
      "1b4aac72459d4e799eb0a3555bc7d707",
      "f17e62fce3704f67ba657bfcf5b60fd3",
      "50e8739619114bcd85ee30eaf761d6f7",
      "86c9d0efe88441889638e606b154eb10",
      "ff408f412b6e4309814cda1cfeee4380",
      "c0deb9bc8adb44ba94d298ac4f67c758",
      "e245db23b8d6477aa22c698ca6eb9529",
      "9a6dcf545973486983f39be05e99494a",
      "69327ed5f24044ca9a7f0b2a95385e40",
      "fb07fcf3df04403b892776d7cf765297",
      "4795f404b9384b9bab90e9c9dd4ac4e6",
      "09c3c0570011444c8c3977276d2fa1e4",
      "4fb58c2ea6584f91904ef27417e982b3",
      "b3c5f28714674b4b86621af629967be5",
      "50c50ae26a7e49e3b5987e41f42f2008",
      "7236fa20ba9a44cf88eb31938a7db016",
      "3f709d08ad174af49cec82f4ed4c354a",
      "19e42b48aed849b79e595be19a237e6c",
      "f1f4cc7e705b441fb2c25ac3aeb6b59e",
      "4cceb685fa3c4f178b86ed4684f26703"
     ]
    },
    "id": "PxQ_4cpUA2i-",
    "outputId": "84cdb13f-d994-4c5a-cfb7-9e51455e21b5"
   },
   "outputs": [],
   "source": [
    "### Load model\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "## Loading Model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    # model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "### Enable Inference Optimization\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C28_U4q5A2eR"
   },
   "outputs": [],
   "source": [
    "### Contextual Retrieval\n",
    "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "<document>\n",
    "{doc_content}\n",
    "</document>\n",
    "\n",
    "Here is the chunk we want to situate within the whole document:\n",
    "<chunk>\n",
    "{chunk_content}\n",
    "</chunk>\n",
    "\n",
    "Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "Answer only with the succinct context and nothing else.\n",
    "\n",
    "<answer>\n",
    "\"\"\"\n",
    "\n",
    "def situate_context(doc: str, chunk: str) -> str:\n",
    "  prompt = DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc, chunk_content=chunk)\n",
    "  inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "  outputs = model.generate(**inputs, max_new_tokens=1024, temperature=0.1, do_sample=False)\n",
    "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mKDxtHpFtZi"
   },
   "outputs": [],
   "source": [
    "### function to process document and one chunk for contextual retrieval and returns the it\n",
    "def get_contextual_content(doc: Document, chunk: Document) -> str:\n",
    "  result = situate_context(doc.page_content, chunk.page_content)\n",
    "  contextual_result = result.split('<answer>')[1].split('</answer>')[0].strip()\n",
    "  chunk.metadata['contextualized_content'] = contextual_result.strip()\n",
    "  return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOkuf_umr6Fm"
   },
   "outputs": [],
   "source": [
    "# chunk_count = 0\n",
    "# for t_chunk in dl.get_chunks():\n",
    "#   if t_chunk.metadata['doc_num'] == dl.get_documents()[7].metadata['doc_num']:\n",
    "#     chunk_count += 1\n",
    "# print(chunk_count)\n",
    "\n",
    "# chunk_count = len([t_chunk for t_chunk in dl.get_chunks() if t_chunk.metadata['doc_num'] == dl.get_documents()[7].metadata['doc_num']])\n",
    "# print(chunk_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zx-mCJ2uqrsU"
   },
   "outputs": [],
   "source": [
    "def process_doc_chunk_for_contextual_retrieval(docs: List[Document], chunks: List[Document]):\n",
    "  # process without using parallel threads\n",
    "  for doc in docs:\n",
    "    with tqdm(total=len([t_chunk for t_chunk in chunks\n",
    "              if t_chunk.metadata['doc_num'] == doc.metadata['doc_num']]),\n",
    "              desc=\"Processing chunks\") as pbar:\n",
    "      print(f\"Processing doc: {doc.metadata['doc_num']}\")\n",
    "      for chunk in chunks:\n",
    "        if chunk.metadata['doc_num'] == doc.metadata['doc_num']:\n",
    "          get_contextual_content(doc, chunk)\n",
    "          pbar.update(1)\n",
    "\n",
    "### TODO: Implement parallel threading\n",
    "# parallel_threads = 4\n",
    "# def process_doc_chunk_for_contextual_retrieval(docs: List[Document], chunks: List[Document], parallel_threads):\n",
    "#   with ThreadPoolExecutor(max_workers=parallel_threads) as executor:\n",
    "#     futures = []\n",
    "#     results = []\n",
    "#     for doc in docs[:1]:\n",
    "#       print(f'Document metadata: {doc.metadata}')\n",
    "#       for chunk in chunks[:2]:\n",
    "#         print(f'Chunk metadata: {chunk.metadata}')\n",
    "#         if chunk.metadata['doc_num'] == doc.metadata['doc_num']:\n",
    "#           futures.append(executor.submit(get_contextual_content, doc, chunk))\n",
    "#     for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing chunks\"):\n",
    "#       results.append(future.result())\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iI0n2YAQaPUf"
   },
   "outputs": [],
   "source": [
    "document_number = 29\n",
    "max_doc_number = len(dl.get_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Iv2J_r4Bh4Z",
    "outputId": "05d40ea7-2c42-46b2-a95a-c67849633843"
   },
   "outputs": [],
   "source": [
    "max_doc_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfIC3uoBBq43",
    "outputId": "0f883ab2-0b11-4495-a663-2b3b206a3710"
   },
   "outputs": [],
   "source": [
    "dl.get_documents()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBibLGV9aRAu",
    "outputId": "8611b9a4-85bc-48d3-a6eb-c4074fe90c10"
   },
   "outputs": [],
   "source": [
    "type(dl.get_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5NcHVwWDn-7",
    "outputId": "e4c5fe9e-ee5f-4406-881b-477dfe01ee6b"
   },
   "outputs": [],
   "source": [
    "dl.get_chunks()[606]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUoZKbkJq6bk",
    "outputId": "c7f41988-827e-4838-a5e4-044b0c5a13e9"
   },
   "outputs": [],
   "source": [
    "### Run contextual retrieval\n",
    "## Add checkpoints to ensure no YOLO runs\n",
    "## completes after each document and all its chunks\n",
    "for i in range(document_number, max_doc_number):\n",
    "  process_doc_chunk_for_contextual_retrieval(dl.get_documents()[i:i+1], dl.get_chunks())\n",
    "  if not os.path.exists(os.path.join(save_file_location, f'data--contextual-retrieval-{date_now}-{document_number}.pkl')):\n",
    "    dl.save_to_file(os.path.join(save_file_location, f'data--contextual-retrieval-{date_now}-{document_number}.pkl'))\n",
    "  document_number += 1\n",
    "  print(f'Completed document {i}...saving')\n",
    "\n",
    "  # process_doc_chunk_for_contextual_retrieval(dl.get_documents(), dl.get_chunks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnmUrQeUZiKw"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(save_file_location, f'data--contextual-retrieval-{date_now}.pkl')):\n",
    "  dl.save_to_file(os.path.join(save_file_location, f'data--contextual-retrieval-{date_now}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGQEHzvWNEXa"
   },
   "outputs": [],
   "source": [
    "# SentenceTransformer(\"hkunlp/instructor-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dH0-NKsHNEdE"
   },
   "outputs": [],
   "source": [
    "### Initiate vectorstore\n",
    "client = QdrantClient(\":memory:\")\n",
    "client.create_collection(\n",
    "    collection_name = \"mental_health_db\",\n",
    "    vectors_config = VectorParams(size = 1024, distance = Distance.COSINE)\n",
    ")\n",
    "vector_store = QdrantVectorStore(\n",
    "    client = client,\n",
    "    collection_name = \"mental_health_db\",\n",
    "    embedding = base_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQMoHP81NEaL"
   },
   "outputs": [],
   "source": [
    "### Add\n",
    "# vector_store.add_documents(dl.get_chunks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JWoYuA7D6Gf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2o70oIHNEVK"
   },
   "outputs": [],
   "source": [
    "query = \"What is a financial budget?\"\n",
    "\n",
    "results = vector_store.similarity_search_with_score(query, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6ISS5k0NESN"
   },
   "outputs": [],
   "source": [
    "class BM25:\n",
    "  def __init__(self, text_splitter, base_embeddings, model, tokenizer):\n",
    "    self.text_splitter = text_splitter\n",
    "    self.base_embeddings = base_embeddings\n",
    "    self.model = model\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def get_text_splitter(self):\n",
    "    return self.text_splitter\n",
    "\n",
    "  def get_base_embeddings(self):\n",
    "    return self.base_embeddings\n",
    "\n",
    "  def get_model(self):\n",
    "    return self.model\n",
    "\n",
    "  def create_vectorstores(self, chunks: List[Document]) -> FAISS:\n",
    "    \"\"\"\n",
    "    Create a BM25 index for the given chunks\n",
    "    \"\"\"\n",
    "    return FAISS.from_documents(chunks, self.base_embeddings)\n",
    "\n",
    "  def create_bm25_index(self, chunks: List[Document]) -> BM25Okapi:\n",
    "    \"\"\"\n",
    "    Create a BM25 index for the given chunks\n",
    "    \"\"\"\n",
    "    tokenized_chunks = [chunk.page_content.split() for chunk in chunks]\n",
    "    return BM25Okapi(tokenized_chunks)\n",
    "\n",
    "  def create_flashrank_index(self, vectorstore):\n",
    "    \"\"\"\n",
    "    Create a FlashRank index for the given chunks\n",
    "    # \"\"\"\n",
    "    # ranker = flashrank.Ranker(self.base_embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\":10})\n",
    "    compression_retriever = ContextualCompressionRetriever(base_compressor=FlashrankRerank().model_rebuild(), base_retriever=retriever)\n",
    "    return compression_retriever\n",
    "\n",
    "  def create_bm25_retriever(self, chunks: List[Document]) -> BM25Retriever:\n",
    "    \"\"\"\n",
    "    Create a BM25 index for the given chunks\n",
    "    \"\"\"\n",
    "    bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "    return bm25_retriever\n",
    "\n",
    "  def create_ensemble_retriever_reranker(self, vectorstore, bm25_retriever) ->EnsembleRetriever:\n",
    "    \"\"\"\n",
    "    Create an ensemble retriever for the given chunks\n",
    "    \"\"\"\n",
    "    retriever_vs = vectorstore.as_retriever(search_kwargs={\"k\":10})\n",
    "    bm25_retriever.k = 5\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[retriever_vs, bm25_retriever],\n",
    "        weights=[0.5, 0.5]\n",
    "    )\n",
    "    redundant_filter = EmbeddingsRedundantFilter(self.base_embeddings)\n",
    "    reranker = FlashrankRerank()\n",
    "    pipeline_compressor = DocumentCompressorPipeline(\n",
    "        transformers=[redundant_filter, reranker])\n",
    "    compression_pipeline = ContextualCompressionRetriever(\n",
    "        base_compressor=pipeline_compressor, base_retriever=ensemble_retriever)\n",
    "    return compression_pipeline\n",
    "\n",
    "  @staticmethod\n",
    "  def generate_cache_key(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a cache key for a document\n",
    "    \"\"\"\n",
    "    return hashlib.md5(document.encode()).hexdigest()\n",
    "\n",
    "  def generate_answer(self, query: str, relevant_chunks: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer for the given query and relevant chunks\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"\n",
    "    Based on the following information, please provide a concise and accurate answer to the question.\n",
    "    If the information is not sufficient to answer the question, say so.\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Relevant information:\n",
    "    {chunks}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    full_prompt = prompt.format(query=query, chunks=\"\\n\\n\".join(relevant_chunks))\n",
    "    inputs = self.tokenizer([full_prompt], return_tensors=\"pt\").to(device)\n",
    "    outputs = self.model.generate(**inputs, max_new_tokens=1024, temperature=0.1, do_sample=False)\n",
    "    return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joaJESbb9wyP"
   },
   "outputs": [],
   "source": [
    "bm = BM25(dl.get_text_splitter(), base_embeddings, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWGzhOzN9wqj"
   },
   "outputs": [],
   "source": [
    "contextualized_vectorstore = bm.create_vectorstores(dl.get_chunks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HwJYxG9-NPb"
   },
   "outputs": [],
   "source": [
    "contextual_bm25_index = bm.create_bm25_index(dl.get_chunks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "zeCG-3An-NMb",
    "outputId": "6671ba3f-71f9-4056-8c50-d54a586cccdc"
   },
   "outputs": [],
   "source": [
    "contextualized_reranker = bm.create_flashrank_index(contextualized_vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYetb6Rm-NJ0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYEIK5N5-NHg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbItl2J2-NE8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cl9x_cIBJGD"
   },
   "outputs": [],
   "source": [
    "### Vector Store setup\n",
    "# client = QdrantClient(\":memory:\")\n",
    "# client.create_collection(\n",
    "#     collection_name=\"mental_health_db\",\n",
    "#     vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    "# )\n",
    "# vector_store = QdrantVectorStore(\n",
    "#     client = client,\n",
    "#     collection_name=\"mental_health_db\",\n",
    "#     embedding=base_embeddings,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qwq2LvYCBQ6d"
   },
   "outputs": [],
   "source": [
    "# vector_store.add_documents(txt_splits)\n",
    "# vector_store.add_documents(pdf_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEp2tmKmBQ4I"
   },
   "outputs": [],
   "source": [
    "# query = \"what is a financial budget?\"\n",
    "\n",
    "# results = vector_store.similarity_search_with_score(query, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnBrsUe0COQ9"
   },
   "outputs": [],
   "source": [
    "# for res in results:\n",
    "#   print(res)\n",
    "#   print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4J4gl-mFCOOm"
   },
   "outputs": [],
   "source": [
    "# for res in results:\n",
    "#   print(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmN4esRoow_L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6y7Liv5JHXk"
   },
   "outputs": [],
   "source": [
    "### Simple Test-Time Scaling Technique\n",
    "\n",
    "## Apply Test-Time Scaling (Budget Forcing)\n",
    "def generate_with_budget_forcing(model, tokenizer, prompt, max_thinking_tokens=30, extra_tokens=10):\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "  generate_ids = model.generate(**inputs, max_new_tokens=max_thinking_tokens + extra_tokens)\n",
    "  output_text = tokenizer.decode(generate_ids[0], skip_special_tokens=False)\n",
    "\n",
    "  tokens = output_text.split()\n",
    "  if len(tokens) > max_thinking_token:\n",
    "    truncated = tokens[:max_thinking_tokens] + [\"<end-of-thinking>\"]\n",
    "    return \" \".join(truncated)\n",
    "  return output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "117-uu8jjTij"
   },
   "outputs": [],
   "source": [
    "## Run Inference with Adapted Generation Function\n",
    "prompt = \"what is a financial budget?\"\n",
    "output = generate_with_budget_forcing(model, tokenizer, prompt)\n",
    "print(\"Generated Output:\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
